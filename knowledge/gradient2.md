# VerstÃ¤ndnis von x in Mehrschichtnetzen

In einem mehrschichtigen Perzeptron (MLP) oder jedem anderen tiefen neuronalen Netzwerk verÃ¤ndert sich der "Eingabevektor" x von Schicht zu Schicht. Hier die Details:

## UrsprÃ¼nglicher Eingabevektor

Bei der ersten Schicht des Netzwerks ist der Eingabevektor ğ‘¥x die tatsÃ¤chliche Eingabe in das Netzwerk, also die Trainingsdaten, die dem Netzwerk fÃ¼r die VorwÃ¤rtspropagation gegeben werden.

## Transformierte Eingabe fÃ¼r jede folgende Schicht

FÃ¼r jede nachfolgende Schicht wird der "Eingabevektor" durch die Ausgabe der vorherigen Schicht definiert. Dies bedeutet, dass das, was als x fÃ¼r eine Schicht dient, tatsÃ¤chlich die aktivierten Ausgaben der vorherigen Schicht sind.

Mathematisch ausgedrÃ¼ckt, wenn x(lâˆ’1) der Ausgabevektor der Schicht lâˆ’1 ist, dann wird dieser Vektor durch Gewichte W(l) und Biases b(l) der Schicht l transformiert, und durch eine Aktivierungsfunktion ğœÏƒ geleitet, um x(l), die Eingabe fÃ¼r die Schicht l+1, zu erzeugen.

## Aktualisierung der Gewichte und Biases

WÃ¤hrend des Backpropagation-Prozesses wird âˆ‚L zu âˆ‚x(l) berechnet, um die Ã„nderung des Loss in Bezug auf die Ausgabe der Schicht ğ‘™l zu verstehen. Diese Gradienteninformation wird dann verwendet, um die Gewichte W(l) und Biases b(l) zu aktualisieren, die die Transformation von x(lâˆ’1) zu x(l) verantworten.

## Beispiel

Wenn ein MLP aus drei Schichten besteht und x die initiale Eingabe ist, dann wÃ¤re:

x(1)=Ïƒ(W(1)x+b(1)) die Ausgabe der ersten Schicht und die Eingabe fÃ¼r die zweite Schicht.

x(2)=Ïƒ(W(2)x(1)+b(2)) die Ausgabe der zweiten Schicht und die Eingabe fÃ¼r die dritte Schicht.

x(3)=Ïƒ(W(3)x(2)+b(3)) die Ausgabe der dritten Schicht und der finale Output des Netzwerks fÃ¼r den gegebenen Eingabewert ğ‘¥x.

## Schlussfolgerung

In mehrschichtigen Netzen ist der Eingabevektor fÃ¼r jede Schicht das Ergebnis der Transformation der Eingabe durch die vorherige Schicht, angepasst durch Gewichte und Biases und modifiziert durch die Aktivierungsfunktion. Dieser Prozess erlaubt es dem Netzwerk, zunehmend komplexere Muster und Beziehungen in den Daten zu lernen und zu modellieren.

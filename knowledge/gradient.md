# Ableitung der Loss-Funktion nach dem Output ğ‘œo des Netzes

Dies ist der erste Schritt, bei dem der Gradient der Loss-Funktion direkt am Output des Netzes berechnet wird. Diese Ableitung bewertet, wie sich kleine VerÃ¤nderungen im Output auf den berechneten Loss auswirken.

## Ableitung der Loss-Funktion nach den Gewichten ğ‘ŠW und Biases ğ‘b

### FÃ¼r Gewichte:

Nachdem Sie den Gradienten der Loss-Funktion bezÃ¼glich des Outputs haben (den Sie mit Ableitung der Kostenfunktion nach Output loss_grad(o, y) berechnen), mÃ¼ssen Sie als NÃ¤chstes herausfinden, wie sich Ã„nderungen an jedem Gewicht auf den Output auswirken, und somit indirekt auf den Loss. Dies wird durch die Multiplikation des Eingabevektors x (fuer mehrere Schichten der Eingabevektor jeder Schicht) mit diesem Gradienten erreicht, was Ihnen den Gradienten jedes Gewichts gibt. Dies entspricht der Kettenregel in der Differentialrechnung, wobei Sie die Ã„nderung des Outputs durch eine Ã„nderung der Gewichte  und dann die Ã„nderung des Loss durch eine Ã„nderung des Outputs betrachten.

### FÃ¼r Biases:

Da der Einfluss eines Bias auf den Output unabhÃ¤ngig vom Wert der Eingabe ist (jeder Bias wird direkt zum Ausgabewert des Neurons addiert), ist der Gradient des Loss bezÃ¼glich eines Bias gleich dem Gradienten des Loss bezÃ¼glich des Outputs des Neurons.

## Backpropagation fÃ¼r vorherige Schichten (wenn vorhanden)

Wenn das Netzwerk mehrere Schichten hat, wird der berechnete Gradient des Outputs verwendet, um rÃ¼ckwÃ¤rts durch das Netzwerk zu gehen und die Gradienten fÃ¼r alle vorigen Schichten zu berechnen. Das beinhaltet die Berechnung der Ableitungen der Ausgaben der versteckten Schichten und die Aktualisierung ihrer Gewichte und Biases basierend auf ihren spezifischen BeitrÃ¤gen zum Output und somit zum Fehler.

## Zusammenfassung

In einem trainierten neuronalen Netzwerk fÃ¼hrt die Berechnung der Ableitungen durch die gesamte Architektur (alle Schichten). Die Backpropagation nutzt die Kettenregel, um den Einfluss jeder Gewichts- und Bias-Ã„nderung in jeder Schicht auf den Endloss zu bestimmen. Die endgÃ¼ltige Aktualisierung der Gewichte und Biases mit ihren berechneten Gradienten verbessert sukzessive die Netzwerkperformance, indem sie den Gesamtloss minimiert. Dieser Prozess wird iterativ Ã¼ber viele Epochen wiederholt, um das Netzwerk optimal auf die Trainingsdaten abzustimmen.
